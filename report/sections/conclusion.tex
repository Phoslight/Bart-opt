Given the results shown above, for our proposed optimizations and pipelines, we believe the results still have potential for further enhancement, beyond the above-mentioned improvements.
For instance, ONNX provides a framework for efficiently accelerating INT8 matrix multiplication, offering strong compatibility with various runtimes. Perhaps ONNX can help achieve faster inference speeds using quantized models.
Additionally, our machine learning process is based entirely on the Python programming language, which is relatively slow in performance since Python only has an interpreter. Although most critical operations in PyTorch are written in pure C++, we still cannot ignore the penalty caused by Python itself. To compensate for this penalty, perhaps we can leverage JIT compilations embedded in PyTorch and compile the models into TorchScripts, thus avoiding the Python interpreter as much as possible to accelerate.
Last but not least, regarding the fine-tuning process, perhaps we can apply LORA (low-rank adaptation), which splits weights into two low-rank matrices to accelerate gradient updates during backward propagation across layers without losing much precision.
In summary, future optimizations should focus on exploring more sophisticated methods that balance computational efficiency and model precision and leveraging emerging advancements in model compression to push the boundaries of the performance of lightweight Transformers.
