Text generation systems play a critical role in improving user efficiency in applications like typing assistants,
chatbots, and new digests.
Traditional methods, often based on n-gram or rule-based models, struggle with understanding complex sentences
and providing accurate suggestions over longer contexts.
The emergence of transformer-based models has transformed Natural Language Processing (NLP).
Models like BERT, BART, and GPT-2 leverage bidirectional context, enabling more accurate predictions.
Transformers also benefit from large pre-trained models and extensive datasets,
but they come with the drawback of requiring significant computational resources,
which can limit their use in real-time applications.

Many companies, such as Microsoft, Meta, and Apple, have developed their own text completion / summarization systems.
However, these solutions often have limitations, such as device or platform restrictions.
Our motivation is to create a context-aware text completion / summarization system that balances quality,
speed, and efficiency, working across platforms without sacrificing performance.

Regarding these challenges, we employed several established techniques to reduce the size of pre-trained models
and enhance model inference speed, while keeping accuracy loss within acceptable limits.
These techniques include knowledge distillation, movement pruning, and quantization.
In the following sections, we will present the development, internals, and outcomes of the optimized pre-trained models.
Section~\ref{sec:related-work} discusses the related work.
Section~\ref{sec:method} presents knowledge distillation, movement pruning, and quantization
(including post-training quantization, PTQ and quantization-aware training, QAT) methodology.
Section~\ref{sec:experiment-results} shares the knowledge distiller, movement pruning techniques, and quantization implementations.
It also summarizes the final and overall performance, inference time,
and memory footprint generated and evaluated by using our distillation-pruning-quantization pipelines
on pre-trained Transformer models.
Section~\ref{sec:conclusions-and-future-work} concludes this project and discusses various potential future improvement
we have observed and summarized in these months during the project.
