# Bart model optimization experiments

Distillations, movement pruning, and quantization (PTQ and QAT) on text summarization using pre-trained bart model.

## Environment

```bash
conda env create -f requirements.yaml -n tmp
conda activate tmp
```

## Bart-large-CNN pipeline
[Bart-large-CNN notebook and results](https://github.com/Phoslight/Bart-opt/blob/main/summarization/pipeline_summarize.ipynb)

## GPT-2 pipeline
[GPT-2 notebook and results](https://github.com/Phoslight/Bart-opt/blob/main/prediction/pipeline_prediction.ipynb)

## Report
[Project report](https://github.com/Phoslight/Bart-opt/blob/main/docs/cap6617fa24_project_Six%20Degrees%20of%20Inner%20Turbulence.pdf)